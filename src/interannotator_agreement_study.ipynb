{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Krippendorff's Alpha\n",
    "\n",
    "This notebook demonstrates how we calculated Krippendorff's alpha for interannotator agreement for our study. \n",
    "\n",
    "Krippendorff's alpha is an ideal choice for our project because it robustly handles missing data and is designed for to handle agreement among multiple annotators, whereas some methods are limited to only two annotators. In our case, where items have nominal categories and varying numbers of annotators (ranging from one to four), Krippendorff's alpha accommodates these inconsistencies without biasing the results. Unlike other agreement metrics that require complete data from all annotators, this method calculates reliability even when some annotations are missing, ensuring that our interannotator agreement assessment remains statistically sound and valid despite the inherent variability in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.metrics.agreement import AnnotationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/timsmac/DSCI/COLX523_linkedin_corpus/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_1  label_2  label_3  label_4\n",
       "0      4.0      6.0      6.0      NaN\n",
       "1      NaN      5.0      4.0      6.0\n",
       "2      6.0      6.0      5.0      NaN\n",
       "3      NaN      5.0      5.0      6.0\n",
       "4      6.0      6.0      6.0      NaN\n",
       "5      6.0      5.0      NaN      6.0\n",
       "6      6.0      1.0      NaN      1.0\n",
       "7      NaN      5.0      3.0      6.0\n",
       "8      6.0      NaN      NaN      6.0\n",
       "9      5.0      NaN      6.0      6.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the data - just the individual annotations\n",
    "\n",
    "# Change this directory as needed\n",
    "annotation_dir = '../data/Annotation_instances/linkedin_combined_annotation.csv'\n",
    "\n",
    "df = pd.read_csv(annotation_dir)[['label_1','label_2','label_3','label_4']].astype(float)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance function\n",
    "\n",
    "For this task, there are several things to take into account when consdering the distance function we should use for Krippendorf's alpha. Namely, some of our labels/categories have significant ambiguity and/or overlap with each other. Thus, inter-annotator disagreements between these categories ought to be less penalized due to the natural ambiguity of our dataset. On the other hand, some categories are quite distant, and the reverse is true for these. In our case, we also have a 'Others' category, which requires its own special treatment as a 'neutral/default' category. As a group, we discussed this and determined that the following categories required some special treatment with the distance function:\n",
    "\n",
    "### Categories 2 and 3:\n",
    "Category 2 (Events) and Category 3 (Interactive Promotions) are different enough in theory to keep as different categories; however in practice, many Linkedin posts from our dataset were quite ambiguous between these two categories. Consider the following example:\n",
    "\n",
    "_'NEW LIVE CLASS: How to land a job you love. Join us on Thursday, June 4th at 11am ET.  Pay what you can and register at  https://bit.ly/2ZQSrlY \\n \\n \\n …see more'_\n",
    "\n",
    "This example demonstrates the overlap; it is largely talking about an event/gathering, but it is soliciting payment for a service and comes from an account that regularly offers paid seminars / is promoting its brand, making it closely related to category 3 as well. Ultimately the above example was labeled as 2, but if somebody thought this was a 3 we believe that sort of disagreement should not be heavily penalized.\n",
    "\n",
    "### Categories 4 and 5:\n",
    "Category 4 (Educational Resources) and Category 5 (Trends) also shared some overlap. Many posts on Linkedin are naturally ambiguous between these two categories since they often pair real-world trends with educational resources. Consider this example:\n",
    "\n",
    "_'Voices of educators as to how to improve online learning.  A worthy read ..... myriad of suggestions.  https://lnkd.in/e2QMwrZ \\n \\n \\n …see more'_\n",
    "\n",
    "This was labeled a 4 by our group, since it does offer an external resource to learn from. However, that resource is clearly centered around an industry trend (the move toward online learning), offering a bit of ambiguity. We found that there were many such examples that created some ambiguity in labeling, so we also decided to be less punitive to errors between categories 4 and 5.\n",
    "\n",
    "### Category 6:\n",
    "Category 6 (Others) is a default, 'catch-all' category for any posts that are too difficult to categorize in the other categories. We discussed how this affects its role in the distance function, and determined that disagreements between this label and each other label should be slightly penalized. Since category 6 has no content of its own and serves only to categorize posts that don't suit another label, it presents a problem if some annotators are routinely using this label when another label truly does apply, and if this is happening frequently it means we have a problem with our schema or the task description, which should be reflected in a lower alpha. Thus, we opted to add a slight penalty to disagreements where either annotator labels a 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_distance(a, b):\n",
    "    # Return 0 if the categories are the same or one is None\n",
    "    if a == b:\n",
    "        return 0\n",
    "    if a is None or b is None: # One annotator is missing from each example; don't penalize this\n",
    "        return 0\n",
    "    # Small penalty between 4/5 , 2/3:\n",
    "    if (a, b) in [(4.0, 5.0), (5.0, 4.0), (2.0, 3.0),(3.0, 2.0)]:\n",
    "        return 0.5\n",
    "    # Larger penalty between any rating and 6:\n",
    "    elif (a == 6.0 and b != 6.0) or (a!= 6.0 and b == 6.0):\n",
    "        return 1.5\n",
    "    # Otherwise, use the default penalty.\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['label_1', 0, np.float64(4.0)],\n",
       " ['label_1', 1, None],\n",
       " ['label_1', 2, np.float64(6.0)],\n",
       " ['label_1', 3, None],\n",
       " ['label_1', 4, np.float64(6.0)],\n",
       " ['label_1', 5, np.float64(6.0)],\n",
       " ['label_1', 6, np.float64(6.0)],\n",
       " ['label_1', 7, None],\n",
       " ['label_1', 8, np.float64(6.0)],\n",
       " ['label_1', 9, np.float64(5.0)]]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting our data into triples, suited for use with NLTK's AnnotationTask package\n",
    "\n",
    "def convert_to_triples(data):\n",
    "    triple_list = []\n",
    "    annotators = ['label_1','label_2','label_3','label_4']\n",
    "    for annotator in annotators:\n",
    "        for i in range (len(data)):\n",
    "            value = data[annotator][i]\n",
    "            if pd.isnull(value):\n",
    "                value = None\n",
    "            else:\n",
    "                value = value\n",
    "            triple_list.append([annotator,i,value])\n",
    "    return triple_list\n",
    "\n",
    "triples = convert_to_triples(data=df)\n",
    "\n",
    "triples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_task = AnnotationTask(triples, distance=custom_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4400713149516744"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement_task.alpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "\n",
    "A Krippendorff’s alpha of `0.44` indicates a moderate level of agreement among annotators, which is understandable given the inherent complexity of the task. In challenging annotation tasks, even experienced annotators can differ substantially in interpretation. On our task in particular, the data is in a lengthy text format, and requires in-depth, nuanced reading of hundreds of examples; further, as already discussed, there is natural ambiguity in the dataset, which can further contribute to a modest alpha score.\n",
    "\n",
    "This level of reliability suggests that while there is some consensus, there remains considerable subjectivity and possibly ambiguity in the annotation guidelines or the task itself. The result provides a useful benchmark, indicating that further refinement in our schema or additional annotator training might improve consistency in future iterations of this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colx521",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
